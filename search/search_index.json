{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83e\udde0 ai-examples","text":"<p>A curated collection of practical AI and LLM examples using Keras,LangChain,OpenAI, Azure OpenAI, and LangGraph. This repository demonstrates how to build intelligent agents, integrate custom tools, work with APIs, and run local models like Mistral using Ollama.</p>"},{"location":"#list-of-examples","title":"\ud83d\udd0d List of examples","text":"\ud83d\udd27 Project Name \ud83d\udccb Description \ud83d\udcf7 basic-cnn Basic CNN model to perform image classification \ud83d\udd01 basic-rnn Simple RNN model to predict next word \ud83d\udd24 tiny-gpt-model Tiny GPT model for learning Transformer architecture \ud83d\udd24 tiny-moe-based-model Tiny sample model based on MoE architecture for predicting the next word \ud83e\udde9 tiny-nested-learning Compare a tiny Transformer with a HOPE-style continuum memory model \ud83e\udd16 local-llm Run local LLM using Ollama \ud83e\uddee basic-agent Basic calculation agent using LangChain and tool \ud83d\udef0\ufe0f basic-agent-tracing-langsmith Calculation agent with LangSmith tracing enabled \ud83d\udce1 basic-rag Basic RAG system to query weather data over a date range \ud83d\udd01 basic-agent-langgraph Agent using LangGraph, structured tool, and logging \ud83c\udf26\ufe0f fastapi-mcp-api Weather API built using FastAPI as MCP server \ud83d\udd0c test-mcp-client Basic MCP client for testing interactions \ud83d\udd04 basic-agent-2-agent Sample using Agent-to-Agent (A2A) communication protocol \ud83d\uddfa\ufe0f basic-multi-agent-system Simple multi-agent architecture using LangGraph \ud83d\uddfa\ufe0f basic-peft A basic example of Parameter-Efficient Fine-Tuning (PEFT) technique"},{"location":"#motivation","title":"\ud83c\udfaf Motivation","text":"<p>This repository began as a personal learning journey \u2014 a place to experiment, break, fix, and deeply understand the core building blocks of AI and modern software systems.</p> <p>Instead of just reading blogs or watching tutorials, I wanted to learn by doing \u2014 by writing real code, training models from scratch, connecting agents, and seeing firsthand how ideas like RNNs, Transformers, LangChain, and local LLMs actually work in practice.</p> <p>Each project here represents a small \u201caha!\u201d moment \u2014 and a belief that the best way to grow is to keep building, keep iterating, and stay curious.</p> <p>If any of these examples help you learn something new (or spark your own experiments), this repo has done its job. \ud83d\ude80</p>"},{"location":"#credits","title":"\ud83d\udcce Credits","text":"<p>Some images or sample data used in this project were found on the internet and are assumed to be in the public domain or used under fair use for non-commercial purposes.</p> <p>Portions of the code, architecture ideas, and debugging help were assisted by ChatGPT, used as a learning partner and coding assistant throughout the project.</p>"},{"location":"basic-agent-2-agent/","title":"A basic A2A agent","text":"<p>Sample using Agent-to-Agent (A2A) communication protocol</p>"},{"location":"basic-agent-2-agent/#check-agent-details","title":"Check Agent Details","text":"<ul> <li>Run the server using below command.</li> </ul> <pre><code>python main.py\n</code></pre> <ul> <li>Then navigate to the URL <code>http://localhost:9999/.well-known/agent.json</code></li> <li>You should be able to see Agent Details</li> </ul> <pre><code>{\n  \"capabilities\": {\n\n  },\n  \"defaultInputModes\": [\n    \"text\"\n  ],\n  \"defaultOutputModes\": [\n    \"text\"\n  ],\n  \"description\": \"A simple agent that returns weather information\",\n  \"name\": \"Weather agent\",\n  \"skills\": [\n    {\n      \"description\": \"Share weather specific information\",\n      \"examples\": [\n        \"What is the weather of blr?\"\n      ],\n      \"id\": \"weather_agent\",\n      \"name\": \"Weather Agent\",\n      \"tags\": [\n        \"weather info\"\n      ]\n    }\n  ],\n  \"url\": \"http://localhost:9999\",\n  \"version\": \"1.0.0\"\n}\n</code></pre>"},{"location":"basic-agent-2-agent/#run-the-client","title":"Run the Client","text":"<ul> <li>Once the server is up and running.</li> <li>Run the below command to get the response from the server.</li> </ul> <pre><code>python client.py\n</code></pre> <ul> <li>If everything goes fine, you should a response. </li> </ul> <pre><code>{\n  \"id\": \"ccaa00f9-8e23-42bb-9880-995865b13e16\",\n  \"jsonrpc\": \"2.0\",\n  \"result\": {\n    \"kind\": \"message\",\n    \"messageId\": \"c0566785-ddf7-40a2-a6fe-03c0601e44e6\",\n    \"parts\": [\n      {\n        \"kind\": \"text\",\n        \"text\": \"{\\\"city\\\": \\\"BLR\\\", \\\"temp\\\": \\\"10c\\\"}\"\n      }\n    ],\n    \"role\": \"agent\"\n  }\n}\n</code></pre>"},{"location":"basic-agent-2-agent/#visual-explaination","title":"Visual Explaination","text":"<pre><code>flowchart TD\n\n    %% CLIENT SIDE\n    subgraph Client_Agent\n        A1[\"A2ACardResolver&lt;br/&gt;(fetches public agent card)\"]\n        A3[\"Message Payload&lt;br/&gt;'What's the weather in BLR?'\"]\n        A2[\"A2AClient&lt;br/&gt;(sends structured message)\"]\n    end\n\n    %% SERVER SIDE\n    subgraph Server_Agent[\"Server Agent (localhost:9999)\"]\n        B1[\"/.well-known/agent.json&lt;br/&gt;(Public Agent Card)\"]\n        B2[\"Message Receiver&lt;br/&gt;(POST /send-message)\"]\n        B3[\"Business Logic&lt;br/&gt;(Weather lookup / LLM)\"]\n        B4[\"Structured Response&lt;br/&gt;(JSON reply)\"]\n    end\n\n    %% CONNECTIONS\n    A1 --&gt; B1\n    A3 --&gt; A2\n    A2 --&gt; B2\n    B2 --&gt; B3\n    B3 --&gt; B4\n    B4 --&gt; A2</code></pre>"},{"location":"basic-agent-with-langsmith/","title":"Basic Agent Tracing Langsmith","text":""},{"location":"basic-agent-with-langsmith/#tracing-agent-call-with-langsmith","title":"Tracing agent call with LangSmith","text":""},{"location":"basic-agent-with-langsmith/#enable-langsmith","title":"Enable LangSmith","text":"<ul> <li>SignUp https://www.langchain.com/langsmith</li> <li>Create an API key and save it.</li> <li>Export following configuration with details.</li> <li>You could also set this as environment variable.</li> </ul> <pre><code>export LANGCHAIN_PROJECT=\"basic-agent-tracing-langsmith\"\nexport LANGCHAIN_TRACING_V2=true\nexport LANGSMITH_API_KEY=\"&lt;your-langsmith-api-key&gt;\"\n</code></pre>"},{"location":"basic-agent-with-langsmith/#trace-output","title":"Trace Output","text":"<ul> <li>Don't need any special call to any of the tracing API.</li> <li>Every time you run a chain or agent, LangSmith will now automatically capture the trace.</li> </ul>"},{"location":"basic-agent-with-langsmith/#visual-explaination","title":"Visual Explaination","text":"<pre><code>flowchart TD\n\n%% Config &amp; Setup\nA[\ud83d\udd27 Load .env Variables] --&gt; B[\ud83d\udd10 Get GitHub Token &amp; Endpoint]\nB --&gt; C[\ud83e\udde0 Initialize ChatOpenAI&lt;br/&gt;GPT-4.1 via GitHub Inference]\n\n%% Tool Definitions\nsubgraph Tools\n    D1[\u2795 Define Tool: add a, b]\n    D2[\u2796 Define Tool: subtract a, b]\nend\n\n%% Prompt Setup\nC --&gt; E[\ud83d\udcdd Create Prompt Template&lt;br/&gt;with Chat History &amp; Scratchpad]\nE --&gt; F[\ud83d\udd17 Register Tools &amp; Prompt&lt;br/&gt;with LLM Agent]\n\n%% Agent &amp; Execution\nF --&gt; G[\ud83e\udd16 Create OpenAI Tools Agent]\nG --&gt; H[\ud83d\ude80 agent_executor.invoke&lt;br/&gt;with user input]\n\n%% Execution Flow\nH --&gt; I[\ud83e\udde0 Agent Selects Tool&lt;br/&gt;based on prompt + input]\nI --&gt; J[\ud83d\udee0\ufe0f Execute Tool&lt;br/&gt;e.g., subtract 10, 3]\nJ --&gt; K[\ud83d\udcac Format Final Answer&lt;br/&gt;from Tool Result]\n\n%% Output and Tracing\nK --&gt; L[\ud83d\udce4 Return Answer to User]\nL --&gt; M[\ud83d\udcc8 Trace sent to LangSmith&lt;br/&gt;if env configured]\n\n%% Style Definitions\nclassDef llm fill:#E1F5FE,stroke:#0288D1,stroke-width:2px;</code></pre>"},{"location":"basic-agent/","title":"Basic Agent flow","text":""},{"location":"basic-agent/#prerequisite","title":"Prerequisite","text":"<p>Before running this project, make sure the following are in place:</p> <p>To run this agent, you'll need a GitHub token (used here as a secret key to access an OpenAI-compatible endpoint):</p>"},{"location":"basic-agent/#follow-these-steps","title":"\ud83d\udc49 Follow these steps:","text":"<ol> <li>Go to GitHub Token Settings https://github.com/settings/tokens</li> <li>Click <code>Generate new token (classic)</code></li> <li>Give it a name</li> <li>Select scopes based on what's needed:</li> <li>For this sample (as an API key), you typically don\u2019t need any scopes unless the endpoint requires GitHub access.</li> <li>Generate the token and copy it \u2014 you won\u2019t see it again.</li> </ol>"},{"location":"basic-agent/#how-to-run-this-project","title":"How to run this project","text":"<ol> <li>Make sure you have python <code>3.12</code> installed in your local learning environment. </li> <li> <p>You'll need a <code>.env</code> file in the root directory with the following:</p> <pre><code>GITHUB_TOKEN=your-access-token-here\n</code></pre> <p>[!NOTE]  This token is used as an API key to authenticate with the custom OpenAI-compatible endpoint.</p> </li> </ol>"},{"location":"basic-agent/#visual-explaination","title":"Visual Explaination","text":"<pre><code>flowchart TD\n    A[\"\ud83e\uddd1 User Query\\ne.g., 'What is 10 - 3?'\"] --&gt; B[\"AgentExecutor\\n(LangChain)\"]\n    B --&gt; C[\"create_openai_tools_agent()\\n(LLM + tools + prompt)\"]\n    C --&gt; D1[\"\ud83e\udde0 LLM\\nGPT-4.1 API (custom endpoint)\"]\n    C --&gt; D2[\"\ud83e\uddf0 Tools\\nadd(a, b), subtract(a, b)\"]\n    D1 &amp; D2 --&gt; E[\"\ud83d\udcdd Final Answer\\ne.g., '7'\"]\n</code></pre>"},{"location":"basic-cnn/","title":"Basic CNN","text":""},{"location":"basic-cnn/#basic-cnn-model","title":"Basic CNN model","text":""},{"location":"basic-cnn/#prerequisite","title":"Prerequisite","text":"<ul> <li>For downloading the MNIST dataset under tensorflow.keras</li> </ul> <pre><code>open /Applications/Python\\ 3.x/Install\\ Certificates.command\n</code></pre>"},{"location":"basic-cnn/#model-training","title":"Model Training","text":"<ul> <li>Train the model using below command:</li> </ul> <pre><code>python model.py\n</code></pre> <ul> <li>Model gets saved as <code>keras</code> format.</li> </ul>"},{"location":"basic-cnn/#evaluate-model","title":"Evaluate Model","text":"<ul> <li>Test the model using below command:</li> </ul> <pre><code>python evaluate.py\n</code></pre>"},{"location":"basic-cnn/#visual-explaination","title":"Visual Explaination","text":""},{"location":"basic-cnn/#training","title":"Training","text":"<pre><code>flowchart TD\n    A[Load MNIST Dataset] --&gt; B[Preprocess Data]\n    B --&gt; C[Encode Labels]\n    C --&gt; D[Define CNN Model]\n    D --&gt; E[Compile Model]\n    E --&gt; F[Train Model]\n    F --&gt; G[Evaluate on Test Set]\n    G --&gt; H[Save Trained Model]    </code></pre>"},{"location":"basic-cnn/#testing","title":"Testing","text":"<pre><code>flowchart TD\n    A[Load Trained Model] --&gt; B[Load Test Image]\n    B --&gt; C[Preprocess Image]\n    C --&gt; D[Make Prediction]\n    D --&gt; E[Map Class Index to Label]\n    E --&gt; F[Print Predicted Letter]</code></pre>"},{"location":"basic-llm-security-proxy/","title":"\ud83d\udee1\ufe0f Basic LLM Security Guarded Proxy","text":"<p>A simple proxy that shows how enterprises can use AI to protect AI by enforcing finance/security policies and AI guardrails (via Llama Guard) before queries reach the LLM.</p> <p>It uses policy-based regex filtering and Llama Guard 3 (8B) via Ollama as a content safety classifier. If a query passes all checks, it is routed to a downstream LLM (e.g., GPT, Claude, or any backend).</p> <p>\ud83d\udeab Blocks unsafe queries like:</p> <ul> <li>Asking for passwords / OTPs / account numbers</li> <li>Fraudulent investment advice  </li> <li>Scams, illegal activity, toxic or harmful content  </li> </ul> <p>\u2705 Allows safe financial education queries such as:</p> <ul> <li>\"Explain compound interest\" </li> <li>\"What are the benefits of a SIP in mutual funds?\" </li> </ul>"},{"location":"basic-llm-security-proxy/#architecture","title":"\ud83d\udcd0 Architecture","text":"<pre><code>flowchart TD\n    User[\"\ud83d\udcbb User Query\"] --&gt; Proxy[\"\ud83d\udee1\ufe0f Guarded Proxy (FastAPI)\"]\n    Proxy --&gt; Regex[\"\ud83d\udd0d Regex Policy Check\"]\n    Proxy --&gt; Guard[\"\ud83e\udd16 Llama Guard 3 (Ollama)\"]\n    Regex --&gt;|Blocked| Deny[\"\u274c Blocked Response\"]\n    Guard --&gt;|Unsafe| Deny\n    Guard --&gt;|Safe| LLM[\"\ud83e\udde0 Downstream LLM (Mock/GPT-4.1/etc.)\"]\n    LLM --&gt; Response[\"\u2705 Allowed Response\"]\n    Deny --&gt; Response</code></pre>"},{"location":"basic-llm-security-proxy/#run-the-project","title":"\ud83d\udce6 Run the project","text":""},{"location":"basic-llm-security-proxy/#download-the-code-locally","title":"Download the code locally","text":"<ol> <li>Clone this repository</li> <li><code>cd basic-llm-security-proxy</code></li> </ol>"},{"location":"basic-llm-security-proxy/#run-model-locally","title":"Run model locally","text":"<pre><code>ollama serve\nollama pull llama-guard3:8b\n</code></pre>"},{"location":"basic-llm-security-proxy/#run-the-proxy-server-locally","title":"Run the proxy server locally","text":"<pre><code>uvicorn proxy_server:app --reload --port 8000\n</code></pre>"},{"location":"basic-llm-security-proxy/#make-api-call-using-curl","title":"Make API call using CURL","text":""},{"location":"basic-llm-security-proxy/#safe","title":"Safe","text":"<pre><code>curl -X POST \"http://127.0.0.1:8000/query\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"query\": \"What is the latest news about stock markets today?\"}'\n</code></pre>"},{"location":"basic-llm-security-proxy/#unsafe","title":"Unsafe","text":""},{"location":"basic-llm-security-proxy/#blocked-by-regex-policy","title":"Blocked by regex policy","text":"<pre><code>curl -X POST \"http://127.0.0.1:8000/query\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"query\": \"Please give me your password so I can invest for you\"}'\n</code></pre>"},{"location":"basic-llm-security-proxy/#blocked-by-llama-guard","title":"Blocked by Llama Guard","text":"<pre><code>curl -X POST \"http://127.0.0.1:8000/query\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"query\": \"How to evade taxes legally?\"}'\n</code></pre> <pre><code>curl -s -X POST \"http://127.0.0.1:8000/query\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"How can I launder money without getting caught?\"}'\n</code></pre>"},{"location":"basic-llm-security-proxy/#output","title":"Output","text":"<pre><code>{\n  \"allowed\": false,\n  \"reason\": \"Blocked by Llama Guard\",\n  \"details\": {\n    \"safe\": false,\n    \"label\": \"unsafe\",\n    \"reason\": \"ollama_classification\",\n    \"score\": 0.9,\n    \"raw_output\": \"unsafe\\nS2\"\n  }\n}\n</code></pre>"},{"location":"basic-peft/","title":"Basic Parameter Efficient Fine Tuning (PEFT)","text":"<p>An example of Parameter-Efficient Fine-Tuning (PEFT) by adding LoRA adapters to a Hugging Face <code>TFDistilBertForSequenceClassification</code> model \u2013 all in pure <code>TensorFlow/Keras</code>.</p> <p></p>"},{"location":"basic-peft/#process-followed","title":"Process followed","text":"<ol> <li>Took a pretrained DistilBERT model for text classification</li> <li>Froze most of the transformer layers to keep training efficient</li> <li>Injected LoRA adapters into only the last two transformer layers</li> <li>Fine-tuned on a small custom dataset with string labels</li> <li>Saved the model, tokenizer, and label map for later inference</li> <li>Perform inferening using the loaded model</li> </ol>"},{"location":"basic-peft/#different-ways-to-fine-tune","title":"Different ways to fine tune","text":"<ol> <li>Self - supervised</li> <li>Supervised \u2705</li> <li>Re-informcement learning</li> </ol>"},{"location":"basic-peft/#different-options-for-parameter-tuning","title":"Different options for parameter tuning","text":"<ol> <li>Retrain all parameters</li> <li>Transfer learning</li> <li>Parameter Efficient Fine-tuning(PEFT) \u2705</li> </ol>"},{"location":"basic-peft/#why-low-rank-adaptation-lora","title":"Why Low-Rank Adaptation (LoRA)?","text":"<p>Instead of fine-tuning the entire 66M+ parameters, LoRA trains only a small fraction by learning low-rank updates. This makes:</p> <ul> <li>Training faster</li> <li>Memory-friendly</li> <li>Ideal for small datasets and CPU/GPU-constrained setups</li> </ul> <p>LoRA (Low-Rank Adaptation) is just one of several Parameter-Efficient Fine-Tuning (PEFT) methods used to adapt large language models without updating all parameters. Other common approaches include:</p> <p>Prefix Tuning \u2013 Learn special prefix tokens for each task while keeping the model frozen.</p> <p>Prompt Tuning \u2013 Learn continuous prompt embeddings instead of discrete prompts.</p> <p>P-Tuning v2 \u2013 Scales prompt tuning to deep models by inserting learned prompts in multiple layers.</p> <p>Adapter Layers \u2013 Insert small trainable modules between transformer layers.</p> <p>BitFit \u2013 Only fine-tune bias terms in the model.</p> <p>These techniques trade off between training cost, parameter count, and task performance, making them great for edge devices or constrained environments.</p>"},{"location":"basic-peft/#usage","title":"Usage","text":""},{"location":"basic-peft/#get-a-hugging-face-token","title":"Get a Hugging Face token","text":"<ol> <li>Go to: <code>https://huggingface.co/settings/tokens</code></li> <li>Create a new Read token</li> <li>Copy it</li> </ol>"},{"location":"basic-peft/#create-sample-data","title":"Create sample data","text":"<pre><code>You are data generator. And you need to generate a training dataset on latest AI topics. Think about Jon and Sumit are talking about the latest AI topics and trends in different categories like (news, opinion, news, comparison) and you need to generate such 100 rows and the output data format as per below:\n\nCORPUS = [\n    {\"text\": \"Jon: Have you seen the latest Anthropic release? Sumit: Yes, it\u2019s impressive.\", \"label\": \"news\"},\n    {\"text\": \"Jon: I think the new GPT update is overhyped. Sumit: I disagree, it\u2019s useful.\", \"label\": \"opinion\"},\n]\n</code></pre>"},{"location":"basic-peft/#run-the-model","title":"Run the model","text":"<pre><code>python model.py\n</code></pre> <pre><code>flowchart TD\n    A[Start] --&gt; B[Import Libraries &amp; Data]\n    B --&gt; C[Define LoRALayer class]\n    C --&gt; D[Extract texts and labels from CORPUS]\n    D --&gt; E[Create label map string-&gt;int]\n    E --&gt; F[Convert labels to integers]\n    F --&gt; G[Load DistilBERT tokenizer]\n    G --&gt; H[Tokenize texts max_length=64]\n    H --&gt; I[Convert to int32 tensors]\n    I --&gt; J[Create tf.data.Dataset]\n    J --&gt; K[Shuffle &amp; Split into train and val sets]\n    K --&gt; L[Load DistilBERT model with num_labels]\n    L --&gt; M[Freeze base transformer layers]\n    M --&gt; N[Inject LoRA into last 2 transformer layers]\n    N --&gt; O[Make classification head trainable]\n    O --&gt; P[Compile model Adam, CrossEntropy]\n    P --&gt; Q[Train model]\n    Q --&gt; R[Check if SAVE_DIR exists -&gt; remove if yes]\n    R --&gt; S[Save model &amp; tokenizer]\n    S --&gt; T[Save label_map.json]\n    T --&gt; U[End]</code></pre>"},{"location":"basic-peft/#perform-prediction-using-save-model","title":"Perform prediction using save model","text":"<pre><code>python serve.py\n</code></pre> <pre><code>flowchart TD\n    A[Start] --&gt; B[Load Tokenizer from MODEL_DIR]\n    B --&gt; C[Load label_map.json]\n    C --&gt; D[Invert label_map to inv_label_map]\n    D --&gt; E[Load Trained DistilBERT Model from MODEL_DIR]\n    E --&gt; F[Define predict_single]\n    F --&gt; G[Encode text with tokenizer]\n    G --&gt; H[Convert tensors to int32]\n    H --&gt; I[Run model forward pass]\n    I --&gt; J[Extract logits]\n    J --&gt; K[Apply softmax to get probabilities]\n    K --&gt; L[Find predicted class index]\n    L --&gt; M[Map index to label using inv_label_map]\n    M --&gt; N[Return label and confidence]</code></pre>"},{"location":"basic-rag/","title":"Basic Rag application","text":"<p>A simple app shows the basic capability of RAG system.</p> <p>You should follow the below steps to run the application:</p>"},{"location":"basic-rag/#step-1-load-the-data","title":"Step 1: Load the data","text":"<ul> <li> <p>Navigate to the <code>basic-rag</code> directory.</p> </li> <li> <p>Run the below command to load the data in a vector store.</p> <pre><code>python load_data.py\n</code></pre> </li> </ul>"},{"location":"basic-rag/#step-2-query-the-llm-model-with-context","title":"Step 2: Query the LLM model with context","text":"<pre><code>python main.py\n</code></pre>"},{"location":"basic-rag/#visual-explaination","title":"Visual Explaination","text":"<pre><code>flowchart TD\n\n%% Data Preparation (load_data.py)\nsubgraph A[Data Preparation load_data.py]\n    A1[\"\ud83d\udcc2 Read .txt Files&lt;br/&gt;from ./weather_data\"]:::input\n    A2[\"\u2702\ufe0f Split Text into Chunks\"]:::process\n    A3[\"\ud83e\udde0 Generate Embeddings&lt;br/&gt;(Azure AI Inference)\"]:::embedding\n    A4[\"\ud83d\udcbe Store Embeddings in&lt;br/&gt;Chroma Vector DB\"]:::vector\n    A1 --&gt; A2 --&gt; A3 --&gt; A4\nend\n\n%% Query &amp; Retrieval (main.py)\nsubgraph B[Query &amp; Retrieval main.py]\n    B1[\"\\\u2753 User Question Input\"]:::input\n    B2[\"\ud83e\udde0 Embed Query&lt;br/&gt;(Azure AI Inference)\"]:::embedding\n    B3[\"\ud83d\udd0d Search Chroma Vector DB&lt;br/&gt;(Top-k Similarity Search)\"]:::vector\n    B4[\"\ud83d\udcc4 Retrieve Relevant Chunks\"]:::retrieval\n    B5[\"\ud83e\uddfe Format Context with&lt;br/&gt;PromptTemplate\"]:::process\n    B6[\"\ud83e\udd16 Query GPT-4.1&lt;br/&gt;(GitHub Inference)\"]:::llm\n    B7[\"\u2705 Return Final Answer&lt;br/&gt;+ Source Docs\"]:::output\n    B1 --&gt; B2 --&gt; B3 --&gt; B4 --&gt; B5 --&gt; B6 --&gt; B7\nend\n\n%% Data flow from vector store to retrieval\nA4 --&gt; B3\n\n%% Style Definitions\nclassDef input fill:#FFF3E0,stroke:#FB8C00,stroke-width:2px;\nclassDef process fill:#E0F7FA,stroke:#00ACC1,stroke-width:2px;\nclassDef embedding fill:#FBE9E7,stroke:#FF7043,stroke-width:2px;\nclassDef vector fill:#F1F8E9,stroke:#7CB342,stroke-width:2px;\nclassDef retrieval fill:#FFFDE7,stroke:#FDD835,stroke-width:2px;\nclassDef llm fill:#EDE7F6,stroke:#7E57C2,stroke-width:2px;\nclassDef output fill:#E8F5E9,stroke:#43A047,stroke-width:2px;\n</code></pre>"},{"location":"basic-rag/#output","title":"Output","text":"<pre><code>\ud83d\udd0d Question: What was the maximum temperature in June 2022?\n\ud83d\udca1 Answer: {'query': 'What was the maximum temperature in June 2022?', 'result': 'The maximum temperature in June 2022 was 32.5\u00b0C, recorded on June 6, 2022.', 'source_documents': [Document(id='ebb04be4-b5d5-4783-9a20-63eb09cec0cb', metadata={'source': 'bangalore.txt'}, page_content='time,tavg,tmin,tmax,prcp\\n01-01-2022,21.4,18.5,24.6,3.3\\n02-01-2022,22,17.5,26.4,0\\n03-01-2022,20.6,15.1,26.6,0\\n04-01-2022,20.5,16.4,26.3,0\\n05-01-2022,20.5,14.8,27.4,0\\n06-01-2022,21.1,15.9,27,0\\n07-01-2022,21.4,16.6,27,0\\n08-01-2022,21.6,17.2,27.2,0\\n09-01-2022,21.7,17.3,27.2,0\\n10-01-2022,21.5,16.7,27.3,0.1\\n11-01-2022,21.8,16.7,28.6,0.1\\n12-01-2022,22.4,18.3,28.6,0.4\\n13-01-2022,22.1,17.5,27.4,0.5\\n14-01-2022,21.8,17.8,27.4,0.2\\n15-01-2022,22,16.2,28.6,0\\n16-01-2022,21.9,17.4,27.6,0.2\\n17-01-2022,21.3,16.8,27,0\\n18-01-2022,21.1,15.3,27.1,0\\n19-01-2022,21,16,27,0\\n20-01-2022,21,13,29,0\\n21-01-2022,22.1,14.5,31.4,0\\n22-01-2022,22.1,14.6,30,0\\n23-01-2022,21.9,14.9,30.4,0\\n24-01-2022,21.2,13.4,28.8,0\\n25-01-2022,20.8,13.1,27.4,0\\n26-01-2022,22.2,17.3,28,0.1\\n27-01-2022,21.5,17.9,25.4,1.8\\n28-01-2022,21.1,17,26.3,0\\n29-01-2022,21.2,15.3,27.2,0\\n30-01-2022,23,18.1,29.4,0\\n31-01-2022,23.4,18.3,29.6,0\\n01-02-2022,23.2,17.4,29.8,0\\n02-02-2022,23,16.3,29.6,0\\n03-02-2022,22.7,16.3,28.7,0\\n04-02-2022,22.9,16.5,30.6,0\\n05-02-2022,22.8,15.9,30.8,0\\n06-02-2022,22.7,18.2,28,0\\n07-02-2022,22.6,17.3,28.3,0\\n08-02-2022,22.3,15.7,29.4,0\\n09-02-2022,21.9,15.3,28.8,0\\n10-02-2022,21.9,14.4,30,0\\n11-02-2022,22.2,16.1,29.4,0\\n12-02-2022,22.3,16,29,0\\n13-02-2022,22.2,15.8,28.2,0\\n14-02-2022,22.2,15.2,28.2,0\\n15-02-2022,22.7,17.1,28.4,0\\n16-02-2022,22.6,15.3,28.8,0\\n17-02-2022,23,16.1,30,0\\n18-02-2022,22.5,14.9,29.4,0\\n19-02-2022,22.7,15.8,29.8,0\\n20-02-2022,23.5,15.8,32,0\\n21-02-2022,24.3,16.3,32.4,0\\n22-02-2022,24.7,17.9,31.3,0\\n23-02-2022,23.2,16.4,30.4,0\\n24-02-2022,22.5,15.5,29.4,0\\n25-02-2022,22.9,14.4,31.2,0\\n26-02-2022,22.5,14.9,30.2,0\\n27-02-2022,23.3,16.8,30,0\\n28-02-2022,23.3,16.1,30.6,0\\n01-03-2022,24,16.6,32,0\\n02-03-2022,23.7,16.9,30.4,0\\n03-03-2022,23.5,16.5,30.8,0\\n04-03-2022,24,17.4,31.6,0\\n05-03-2022,24,16.2,30.6,0\\n06-03-2022,23.8,17.2,30.3,0\\n07-03-2022,21.9,16.3,27.9,2.4\\n08-03-2022,24.9,18,30.6,0\\n09-03-2022,24.9,16.7,32.8,0\\n10-03-2022,25.4,17.1,32.8,0\\n11-03-2022,25.2,17.2,32.8,0\\n12-03-2022,25.5,18.1,33,0\\n13-03-2022,25.6,18.5,32.8,0\\n14-03-2022,26,18.4,32.8,0\\n15-03-2022,25.7,17.5,33.4,0\\n16-03-2022,25.7,17.6,34.2,0\\n17-03-2022,26.6,19.1,34.2,0\\n18-03-2022,26.6,18.7,34,0\\n19-03-2022,26.2,18.9,33.4,0.1\\n20-03-2022,24.9,22,34.1,0\\n21-03-2022,24.7,21.3,33.2,6.1\\n22-03-2022,25.6,21.8,33.5,7.9\\n23-03-2022,27,21.4,32.8,2\\n24-03-2022,27.1,23.4,33.6,0\\n25-03-2022,26.4,19.4,32.3,0.2\\n26-03-2022,26.7,19.9,32.9,0\\n27-03-2022,27.1,20.4,33.8,0\\n28-03-2022,27.3,20.1,33.8,0\\n29-03-2022,27.5,20.4,34.2,0.1\\n30-03-2022,27.6,21.8,33.8,0.1\\n31-03-2022,27.5,20.9,34,0\\n01-04-2022,27.2,21.2,33,0\\n02-04-2022,26.8,20,33,0\\n03-04-2022,26.6,18.6,33.6,0\\n04-04-2022,26.5,19,33.8,0\\n05-04-2022,26.3,19,33.2,0\\n06-04-2022,26.7,18.4,35,0\\n07-04-2022,27.1,19.1,34.8,0\\n08-04-2022,26.7,19.3,33.2,0\\n09-04-2022,27.1,19.9,33.6,0\\n10-04-2022,27,20.3,34,0\\n11-04-2022,27.2,20.1,34.2,0\\n12-04-2022,26.7,20.5,34,0\\n13-04-2022,25,21.5,32.8,3.5\\n14-04-2022,24.8,20.7,32.1,13.6\\n15-04-2022,25.4,19.1,33,37.6\\n16-04-2022,26.3,19.7,34.1,0\\n17-04-2022,25.6,20,34.1,51.1\\n18-04-2022,23.8,19.1,32.1,1\\n19-04-2022,26,20,32.4,32.8\\n20-04-2022,27.9,21.8,34.1,0\\n21-04-2022,27.3,20.8,33.4,0\\n22-04-2022,26.7,20.3,33.2,0\\n23-04-2022,26.3,20.5,32.2,0\\n24-04-2022,27,20.4,33.7,0\\n25-04-2022,27.8,20.8,34.1,0\\n26-04-2022,27.9,20.6,34.5,0\\n27-04-2022,28.5,21.5,35.1,0\\n28-04-2022,28.2,21.7,34.8,0\\n29-04-2022,29.1,21.2,36.9,0\\n30-04-2022,28.1,22.7,36.7,0\\n01-05-2022,24.9,20.4,34.8,0\\n02-05-2022,26.3,20.2,33.9,54.1\\n03-05-2022,26.3,21.8,34,0\\n04-05-2022,25.5,20.9,34.2,8.9\\n05-05-2022,23.9,20.4,34.2,4.3\\n06-05-2022,25.2,21.3,31.8,0.3\\n07-05-2022,26,21.2,33.5,5.1\\n08-05-2022,26.1,21,33.2,0\\n09-05-2022,25.3,21,33.2,2\\n10-05-2022,22.6,20.2,31.5,22.9\\n11-05-2022,22.1,20.4,25,5.1\\n12-05-2022,21.3,19.5,25,5.3\\n13-05-2022,22.9,20,26.3,2\\n14-05-2022,25.2,20.3,29.7,0\\n15-05-2022,25.2,20.9,29.7,21.1\\n16-05-2022,23.9,20.8,29.5,0.3\\n17-05-2022,23.8,21.1,30.1,6.4\\n18-05-2022,24.3,20.2,29,114.6\\n19-05-2022,20.6,19.4,29,0.8\\n20-05-2022,21.8,17.9,27,4.8\\n21-05-2022,22.8,18.8,27,3.8\\n22-05-2022,23.8,20.7,28.5,0\\n23-05-2022,24.5,19.9,29.6,0\\n24-05-2022,26.2,22.1,30.7,0\\n25-05-2022,24.6,20.1,30,0\\n26-05-2022,24.9,21.8,31.7,1.1\\n27-05-2022,26.1,21.6,31.9,0\\n28-05-2022,24.5,19.2,30,0.3\\n29-05-2022,24.8,19.5,30,0\\n30-05-2022,23.9,19.6,29.6,12.6\\n31-05-2022,24.1,20.6,30.4,6.9\\n01-06-2022,25.2,21.2,30.4,3.3\\n02-06-2022,25.7,22,31,0\\n03-06-2022,24.1,20.2,31.2,10.7\\n04-06-2022,24.2,19.7,32.2,7.2\\n05-06-2022,24.4,20.4,31.6,17.3\\n06-06-2022,24.8,19.1,32.5,32\\n07-06-2022,24.9,21.1,30.6,0.8\\n08-06-2022,25.1,21.2,30.6,0\\n09-06-2022,24.3,19.8,29.4,1\\n10-06-2022,24,19.5,29.2,1.6\\n11-06-2022,23.6,19.3,29.2,2\\n12-06-2022,23,19.7,28.8,10\\n13-06-2022,23.6,19.2,28.9,0.1\\n14-06-2022,23.6,19.4,29.5,0.6\\n15-06-2022,23.7,20.2,31.9,31\\n16-06-2022,24.5,20,30.6,0\\n17-06-2022,24.6,19.7,30.6,38.9\\n18-06-2022,23.8,21.2,30.5,42.9\\n19-06-2022,24.2,21.1,29.5,1\\n20-06-2022,24.1,20.7,29.5,2\\n21-06-2022,23.2,19.8,27.8,15\\n22-06-2022,22.7,20.4,27.8,0.3\\n23-06-2022,23.1,20.4,26.2,0\\n24-06-2022,22.8,19.5,27.6,0.3\\n25-06-2022,22.4,19.7,27.7,4.2\\n26-06-2022,23.2,20.3,27.2,0.2\\n27-06-2022,23.7,20.1,28.3,0\\n28-06-2022,22.5,19.6,26.7,2.2\\n29-06-2022,23.9,21.1,29.5,0\\n30-06-2022,23.2,20.5,29.5,5.8\\n01-07-2022,23.6,20.5,28.7,3.6\\n02-07-2022,24.1,20.9,28.7,0\\n03-07-2022,23.5,21.1,28.5,0\\n04-07-2022,22.5,20.2,27,0\\n05-07-2022,22.9,20.1,26.5,2\\n06-07-2022,22.1,20.6,26.5,0.3\\n07-07-2022,21.4,19.7,25,1\\n08-07-2022,21.4,19.7,25.5,6.1\\n09-07-2022,21.7,19.8,25.5,7.1\\n10-07-2022,22.4,20,26.2,15\\n11-07-2022,21.7,20.2,26.2,0.8\\n12-07-2022,23.2,19.8,27.5,4.6\\n13-07-2022,21.6,19.7,27.5,1\\n14-07-2022,21.2,19.5,24.3,2\\n15-07-2022,22.3,19.5,25.6,2.3\\n16-07-2022,22,20.1,25.6,0.5\\n17-07-2022,23.2,20.2,25.3,0\\n18-07-2022,22.6,20.9,27.5,0.3\\n19-07-2022,24.7,20.1,29.6,0.3\\n20-07-2022,24.9,19.8,30.8,0\\n21-07-2022,23.7,20.5,30.8,82.5\\n22-07-2022,23.2,21.1,27.9,0\\n23-07-2022,23.1,20.9,26.7,0\\n24-07-2022,22.8,20,26.7,0.3\\n25-07-2022,24.1,20.2,28.5,0.5')]}\n</code></pre>"},{"location":"basic-rnn/","title":"Basic RNN","text":""},{"location":"basic-rnn/#basic-rnn-model","title":"Basic RNN model","text":"<p>Basic RNN for the next Word Prediction</p>"},{"location":"basic-rnn/#model-training","title":"Model Training","text":"<ul> <li>Train the model using below command:</li> </ul> <pre><code>python model.py\n</code></pre> <ul> <li>Model gets saved as <code>keras</code> format.</li> </ul>"},{"location":"basic-rnn/#evaluate-model","title":"Evaluate Model","text":"<ul> <li>Test the model using below command:</li> </ul> <pre><code>python evaluate.py\n</code></pre>"},{"location":"basic-rnn/#visual-explaination","title":"Visual Explaination","text":""},{"location":"basic-rnn/#training","title":"Training","text":"<pre><code>flowchart TD\n    A[Start Script] --&gt; B[Prepare Text Corpus]\n    B --&gt; C[Tokenize Corpus with Keras Tokenizer]\n    C --&gt; D[Generate Input Sequences and Targets]\n    D --&gt; E[Pad Sequences to Fixed Length]\n    E --&gt; F[Convert Sequences to Numpy Arrays]\n\n    F --&gt; G[Build RNN Model with Embedding + SimpleRNN + Dense]\n    G --&gt; H[Compile Model with Adam and Crossentropy Loss]\n    H --&gt; I[Train Model on Input Data]\n    I --&gt; J[Save Trained Model as .keras]</code></pre>"},{"location":"basic-rnn/#testing","title":"Testing","text":"<pre><code>flowchart TD\n    A[Start Script] --&gt; B[Load Trained RNN Model from File]\n    B --&gt; C[Recreate Corpus and Tokenizer]\n    C --&gt; D[Reconstruct Index-to-Word Mapping]\n    D --&gt; E[User Enters Input Phrase]\n    E --&gt; F[Tokenize and Pad the Input Phrase]\n    F --&gt; G[Predict Next Word Using Model]\n    G --&gt; H[Find Predicted Word from Index]\n    H --&gt; I[Display Predicted Word to User]\n    I --&gt; E</code></pre>"},{"location":"fastapi-mcp-api-and-client/","title":"FastAPI MCP Server and Client","text":""},{"location":"fastapi-mcp-api-and-client/#fast-api-server-and-client","title":"Fast API server and client","text":""},{"location":"fastapi-mcp-api-and-client/#run-the-server","title":"Run the server","text":"<ul> <li>Navigate to the <code>fastapi-mcp-api</code> directory.</li> <li> <p>And execute the below command to run the server.</p> <pre><code>uvicorn main:app --reload\n</code></pre> </li> </ul>"},{"location":"fastapi-mcp-api-and-client/#run-the-client","title":"Run the client","text":"<ul> <li>Navigate to the <code>test-mcp-client</code> directory.</li> <li> <p>Then run the below command to execute the client.</p> <pre><code>python main.py\n</code></pre> </li> </ul>"},{"location":"fastapi-mcp-api-and-client/#visual-explaination","title":"Visual Explaination","text":"<pre><code>graph TD\n    subgraph Client Side\n        MainScript[main.py]\n        MCPClient[MCPClient httpx]\n    end\n\n    subgraph FastAPI App\n        App[FastAPI App]\n        Router[routes.weather]\n        InMemoryRepo[InMemoryWeatherRepository]\n        WeatherRepo[WeatherRepository]\n        WeatherModel[Weather]\n    end\n\n    MainScript --&gt; MCPClient\n    MCPClient --&gt;|POST /weathers| App\n    MCPClient --&gt;|GET /weathers| App\n    MCPClient --&gt;|GET /weathers/&lt;'city'&gt;| App\n\n    App --&gt; Router\n    Router --&gt; InMemoryRepo\n    InMemoryRepo --&gt;|implements| WeatherRepo\n    Router --&gt; WeatherModel\n    InMemoryRepo --&gt; WeatherModel\n\n    WeatherModel --&gt;|inherits| Pydantic[BaseModel]</code></pre>"},{"location":"local-llm/","title":"Local LLM","text":""},{"location":"local-llm/#clean-up-script","title":"Clean up script","text":"<pre><code>docker rm -f $(docker ps -aq)\n</code></pre>"},{"location":"local-llm/#visual-explaination","title":"Visual Explaination","text":"<pre><code>flowchart TD\n    A[User or Client Application] --&gt;|HTTP Request| B[Local Ollama Server]\n    B --&gt; C[LLM Runtime Engine\\nMistral Model]\n    C --&gt; B\n    B --&gt;|HTTP Response\\nLLM Output| A\n\n    subgraph Local Environment\n        B\n        C\n    end\n\n    style B fill:#f9f,stroke:#333,stroke-width:2px\n    style C fill:#bbf,stroke:#333,stroke-width:2px\n    style A fill:#cff,stroke:#333,stroke-width:2px\n</code></pre>"},{"location":"multi-agent-system/","title":"Basic Multi Agent Systems","text":""},{"location":"multi-agent-system/#basic-multi-agent-system","title":"Basic Multi Agent System","text":"<p>This project demonstrates a simple yet powerful multi-agent architecture using LangGraph. It simulates collaborative decision-making across three specialized agents:</p> <p>Controller Agent: Validates and interprets the user's input query.</p> <p>Planner Agent: Uses an LLM to generate a structured plan based on the user\u2019s intent.</p> <p>Action Agent: Executes the plan and returns a response, such as the current weather information.</p> <p>Each agent operates independently with a shared memory structure (WeatherState), enabling them to communicate, coordinate, and solve tasks step-by-step. This pattern allows for modular, scalable, and LLM-driven orchestration, ideal for real-world applications like customer support, travel planning, or weather advisory systems.</p>"},{"location":"multi-agent-system/#execute-the-system","title":"Execute the system","text":"<pre><code>python main.py\n</code></pre>"},{"location":"multi-agent-system/#visual-explaination","title":"Visual Explaination","text":""},{"location":"multi-agent-system/#flow-diagram","title":"Flow Diagram","text":"<pre><code>flowchart TD\n    A[User submits weather query]\n    B[main.py calls build_weather_graph]\n    C[Controller Agent processes input]\n    D{Is input weather-related?}\n    E1[Planner Agent uses LLM to create plan]\n    E2[Return error or fallback response]\n    F[Plan stored in state]\n    G[Action Agent executes the plan]\n    H[Action stored in state]\n    I[main.py prints final action]\n    J[Final result returned to user]\n\n    A --&gt; B --&gt; C --&gt; D\n    D -- Yes --&gt; E1 --&gt; F --&gt; G --&gt; H --&gt; I --&gt; J\n    D -- No --&gt; E2 --&gt; J</code></pre>"},{"location":"multi-agent-system/#sequence-diagram","title":"Sequence Diagram","text":"<pre><code>sequenceDiagram\n    participant User as User\n    participant Main as main.py\n    participant Controller as ControllerAgent\n    participant Planner as PlannerAgent (LLM)\n    participant Action as ActionAgent\n    participant Result as Final State\n\n    User-&gt;&gt;Main: Start with query \"What's the weather like in Bangalore?\"\n    Main-&gt;&gt;Controller: Pass WeatherState(input_query)\n    Controller-&gt;&gt;Controller: Print and forward state\n    Controller--&gt;&gt;Main: Return unchanged state\n\n    Main-&gt;&gt;Planner: Pass state.input_query\n    Planner-&gt;&gt;LLM: Generate plan from query\n    LLM--&gt;&gt;Planner: Plan = \"Check weather in Bangalore\"\n    Planner-&gt;&gt;Planner: Update state.plan\n    Planner--&gt;&gt;Main: Return updated state\n\n    Main-&gt;&gt;Action: Pass state.plan\n    Action-&gt;&gt;Action: Simulate execution (e.g., fetch weather)\n    Action-&gt;&gt;Action: Update state.action = \"Executed plan \u2192 Sunny 25\u00b0C in BLR\"\n    Action--&gt;&gt;Main: Return updated state\n\n    Main-&gt;&gt;Result: Final State with action\n    Result--&gt;&gt;User: Print final action response</code></pre>"},{"location":"test-mcp-client/","title":"Test MCP Client","text":"<p>A simple project that showcase how a MCP client and server would work interact with each other.</p>"},{"location":"test-mcp-client/#visual-explaination","title":"Visual Explaination","text":"<pre><code>sequenceDiagram\n    actor User\n    participant MCPClient as MCPClient (client.py)\n    participant Server as Weather API Server (localhost:8000)\n\n    User-&gt;&gt;MCPClient: Run `main()`\n\n    MCPClient-&gt;&gt;Server: POST /weathers {city: \"blr\", temp: \"10c\"}\n    Server--&gt;&gt;MCPClient: 201 Created {city: \"blr\", temp: \"10c\"}\n    MCPClient-&gt;&gt;User: print(\"Added blr weather\")\n\n    MCPClient-&gt;&gt;Server: POST /weathers {city: \"hyd\", temp: \"20c\"}\n    Server--&gt;&gt;MCPClient: 201 Created {city: \"hyd\", temp: \"20c\"}\n    MCPClient-&gt;&gt;User: print(\"Added hyd weather\")\n\n    MCPClient-&gt;&gt;Server: GET /weathers\n    Server--&gt;&gt;MCPClient: 200 OK [List of Weather]\n    MCPClient-&gt;&gt;User: print(\"Fetched all weathers\")\n\n    MCPClient-&gt;&gt;Server: GET /weathers/blr\n    Server--&gt;&gt;MCPClient: 200 OK {city: \"blr\", temp: \"10c\"}\n    MCPClient-&gt;&gt;User: print(\"Fetched BLR weather\")\n\n    MCPClient-&gt;&gt;Server: Close HTTP client</code></pre>"},{"location":"tiny-gpt-model/","title":"A simple tiny GPT model","text":"<p>\ud83d\udccc Note: This model is for educational purposes only. It is trained on a small dataset and is not intended for production use.</p> <ul> <li><code>Tensorflow 2.15.0</code> doesn't support <code>3.12</code> and needs <code>3.11</code></li> <li> <p>Setup the poetry shell with the following command:</p> <pre><code>poetry env use /usr/local/bin/python3.11\n</code></pre> </li> <li> <p>Use the newly created shell</p> <pre><code>poetry shell\n</code></pre> </li> <li> <p>Running the model with character embedding.</p> <pre><code>python model_char_embedding.py\n</code></pre> </li> <li> <p>Running the model with word embedding.</p> <pre><code>python model_word_embedding.py\n</code></pre> </li> </ul>"},{"location":"tiny-gpt-model/#visual-explaination","title":"Visual Explaination","text":"<pre><code>flowchart TD\n    A[Text Corpus] --&gt; B[Tokenization]\n    B --&gt; C[Vocabulary &amp; Sequence Generation]\n    C --&gt; D[Pad Sequences]\n\n    D --&gt; E[Define GPT Model]\n    E --&gt; F[Embedding Layer]\n    F --&gt; G[Self-Attention Block]\n    G --&gt; H[Feedforward Layers]\n\n    H --&gt; I[Compile Model]\n    I --&gt; J[Train Model]\n    J --&gt; K[Generate Text]\n    K --&gt; L[Output Result]</code></pre>"},{"location":"tiny-moe-based-model/","title":"Mixture of Experts (MoE)","text":"<p>Mixture of Experts (MoE) is a model architecture that leverages multiple specialized sub-models \u2014 called experts to process different aspects of the input. Rather than relying on a single, large model to handle all inputs, MoE uses a gating mechanism to dynamically route each input to the most relevant experts. This approach improves both efficiency and scalability.</p>"},{"location":"tiny-moe-based-model/#moe-vs-transformer","title":"MoE Vs Transformer","text":"Feature Mixture of Experts (MoE) Transformer Core Idea Uses multiple expert networks, and a gating mechanism selects a few to activate per input. Relies on self-attention to model relationships between all tokens. Computation Efficiency Sparse computation (only a few experts are active per input) \u2192 Efficient scaling. Dense computation \u2014 all parameters used per forward pass. Scalability Scales well to very large models with reduced computation cost. Scaling increases both compute and memory proportionally. Example Open Source Models - Mixtral - GLaM (Google)- Switch Transformer (Google) - BERT- GPT-2/3 Routing/Selection Gating network decides which experts are active per input token. No routing \u2014 full attention over all tokens in each layer."},{"location":"tiny-moe-based-model/#model-training","title":"Model Training","text":"<ul> <li> <p>Run the below command to do the model training</p> <pre><code>python model.py\n</code></pre> </li> <li> <p>This actually saves all the related model specific weights and parameters in disk under <code>saved_model</code> directory.</p> </li> <li> <p>You would notice there will be primary 3 different files.</p> <ul> <li>Actual model architecture with weights.</li> <li>Tokenizer specification</li> <li>Max Sequence information used in training.</li> </ul> </li> </ul>"},{"location":"tiny-moe-based-model/#model-prediction","title":"Model Prediction","text":"<ul> <li> <p>Run the below command to do the model prediction</p> <pre><code>python serve.py\n</code></pre> </li> </ul>"},{"location":"tiny-moe-based-model/#visual-explaination","title":"Visual Explaination","text":""},{"location":"tiny-moe-based-model/#model-training_1","title":"Model Training","text":"<pre><code>graph TD\n    A[Input Sequence] --&gt; B[Embedding Layer]\n    B --&gt; C[GlobalAveragePooling1D]\n\n    %% Expert Path\n    C --&gt; E1[Expert 1 - Dense\u2192Dense]\n    C --&gt; E2[Expert 2 - Dense\u2192Dense]\n    C --&gt; E3[Expert 3 - Dense\u2192Dense]\n    C --&gt; E4[Expert 4 - Dense\u2192Dense]\n\n    %% Gating Path\n    C --&gt; G[Gating Network Dense \u2192 Softmax]\n\n    %% Weighted Sum\n    E1 --&gt; W\n    E2 --&gt; W\n    E3 --&gt; W\n    E4 --&gt; W\n    G -- gate weights --&gt; W\n\n    %% Final Output\n    W --&gt; O[Final Dense + Softmax&lt;br/&gt; Vocab Prediction]\n\n    %% Styling\n    classDef highlight fill:#6ab04c,stroke:#44bd32,stroke-width:2px;\n    classDef gate stroke-dasharray: 5 5,stroke-width:2px,stroke:#e84118;\n    class W highlight\n    class G,W gate\n</code></pre>"},{"location":"tiny-moe-based-model/#model-serving","title":"Model Serving","text":"<pre><code>graph TD\n    A[Input Text] --&gt; B[Tokenizer]\n    B --&gt; C[Convert to Sequence]\n    C --&gt; D[Pad Sequences&lt;br/&gt;to max_seq_len]\n    D --&gt; E[Load Trained MoE Model&lt;br/&gt; Custom layers - MoELayer + GatingNetwork]\n    E --&gt; F[Predicted Probabilities&lt;br/&gt;Softmax Output]\n    F --&gt; G[np.argmax]\n    G --&gt; H[Predicted Token ID]\n    H --&gt; I[Map to Word&lt;br/&gt;index_word]\n\n    %% Styling\n    classDef preprocess fill:#f9ca24,stroke:#f0932b,stroke-width:2px;\n    classDef model fill:#74b9ff,stroke:#0984e3,stroke-width:2px;\n    classDef postprocess fill:#55efc4,stroke:#00cec9,stroke-width:2px;\n\n    class B,C,D preprocess\n    class E model\n    class F,G,H,I postprocess</code></pre>"},{"location":"tiny-nested-learning/","title":"\ud83e\uddea Tiny Nested Learning Example","text":"<p>A hands-on comparison of Transformers vs HOPE</p> <p>This project is a compact, intuitive demonstration of continual learning \u2014 how a machine learning model behaves when it learns Task A and then Task B, and whether it forgets what it learned earlier.</p> <p>It implements a simplified version of ideas from Google\u2019s Nested Learning research and compares:</p> <ul> <li>a tiny Transformer encoder (baseline attention-only learner)  </li> <li>a tiny HOPE-inspired recurrent model using a continuum memory system (CMS) with fast, medium, and slow update timescales  </li> </ul> <p>Both models are trained on two tiny natural-language tasks:</p> <ul> <li>Task 0 \u2014 Catch a train </li> <li>Task 1 \u2014 Catch a flight</li> </ul> <p>By observing how much each model remembers Task 0 after learning Task 1, we get a clear, human-readable demonstration of catastrophic forgetting and how multi-timescale memory can mitigate it.</p>"},{"location":"tiny-nested-learning/#note","title":"\ud83d\udcdd Note","text":"<p>HOPE is not an acronym. It is the name of a recurrent architecture introduced by Google\u2019s Nested Learning research.</p> <p>HOPE models use a Continuum Memory System (CMS) composed of fast, medium, and slow memory tracks. Each track updates at a different timescale, allowing the model to learn new information while preserving long-term stability.</p> <p>This structure helps HOPE reduce catastrophic forgetting compared to architectures that rely on a single shared state (such as Transformers or standard RNNs).</p> <p>The name \u201cHOPE\u201d reflects the goal of achieving hopeful, continual learning \u2014 retaining older knowledge while integrating new tasks.</p>"},{"location":"tiny-nested-learning/#setup","title":"\ud83d\ude80 Setup","text":"<pre><code>cd tiny-nested-learning\ndocker compose build --no-cache\ndocker compose up\n</code></pre>"},{"location":"tiny-nested-learning/#what-the-script-does","title":"\ud83c\udfaf What the Script Does","text":"<ol> <li>Builds two tiny text tasks from short English stories.  </li> <li>Trains the Transformer on Task 0 \u2192 Task 1.  </li> <li>Trains the HOPE model on the same sequence.  </li> <li>Prints color-coded retention tables to show forgetting vs. retention.</li> </ol>"},{"location":"tiny-nested-learning/#the-two-tiny-tasks","title":"\ud83d\udcd8 The Two Tiny Tasks","text":""},{"location":"tiny-nested-learning/#task-0-catch-a-train","title":"Task 0 \u2014 Catch a Train","text":"<pre><code>i walk to the station with my ticket\ni wait on the platform for the blue train\ni find my seat and watch trees go by\n</code></pre>"},{"location":"tiny-nested-learning/#task-1-catch-a-flight","title":"Task 1 \u2014 Catch a Flight","text":"<pre><code>i take a cab to the busy airport\ni wait in a long line at the gate\ni find my seat and watch clouds go by\n</code></pre>"},{"location":"tiny-nested-learning/#architecture-overview-with-mermaid-visuals","title":"\ud83e\udde0 Architecture Overview (with Mermaid Visuals)","text":""},{"location":"tiny-nested-learning/#transformer-one-shared-memory-system","title":"\ud83d\udd35 Transformer \u2014 One Shared Memory System","text":"<pre><code>flowchart LR\n    subgraph Transformer\n        T0[Token IDs] --&gt; TE[Embedding Layer]\n        TE --&gt; MH[Multi-Head Attention]\n        MH --&gt; LN1[LayerNorm + Residual]\n        LN1 --&gt; FFN[Feed Forward Network]\n        FFN --&gt; LN2[LayerNorm + Residual]\n        LN2 --&gt; LOGITS[Token Logits]\n    end</code></pre>"},{"location":"tiny-nested-learning/#hope-multi-timescale-memory-fast-medium-slow","title":"\ud83d\udfe2 HOPE \u2014 Multi-Timescale Memory (Fast / Medium / Slow)","text":"<pre><code>flowchart LR\n    subgraph HOPE\n        T0[Token IDs] --&gt; EMB[Embedding Layer]\n        EMB --&gt; CMS[CMS Cell&lt;br/&gt;Controller + Rate Adapter]\n\n        CMS --&gt; F[Fast Memory&lt;br/&gt;Update ~ 0.6]\n        CMS --&gt; M[Medium Memory&lt;br/&gt;Update ~ 0.3]\n        CMS --&gt; S[Slow Memory&lt;br/&gt;Update ~ 0.02]\n\n        F &amp; M &amp; S --&gt; AGG[Aggregated State]\n        AGG --&gt; LOGITS[Token Logits]\n    end</code></pre>"},{"location":"tiny-nested-learning/#hope-memory-update-sequence-diagram","title":"\ud83e\udde0 HOPE Memory Update \u2014 Sequence Diagram","text":"<pre><code>sequenceDiagram\n    autonumber\n\n    participant X as Input Token&lt;br/&gt;x_t\n    participant C as Controller\n    participant R as Rate Adapter\n    participant F as Fast Memory\n    participant M as Medium Memory\n    participant S as Slow Memory\n    participant A as Aggregator\n    participant O as Output Logits\n\n    X-&gt;&gt;C: Token embedding\n    C-&gt;&gt;C: Compute candidate&lt;br/&gt;state h_candidate\n\n    C-&gt;&gt;R: Send candidate for&lt;br/&gt;rate computation\n    R-&gt;&gt;F: \u03b1_fast\n    R-&gt;&gt;M: \u03b1_med\n    R-&gt;&gt;S: \u03b1_slow\n\n    Note over F: Update rule:&lt;br/&gt;(1 - \u03b1_fast)*old&lt;br/&gt; + \u03b1_fast*h_candidate\n    Note over M: Update rule:&lt;br/&gt;(1 - \u03b1_med)*old&lt;br/&gt; + \u03b1_med*h_candidate\n    Note over S: Update rule:&lt;br/&gt;(1 - \u03b1_slow)*old&lt;br/&gt; + \u03b1_slow*h_candidate\n\n    F-&gt;&gt;A: Updated fast memory\n    M-&gt;&gt;A: Updated med memory\n    S-&gt;&gt;A: Updated slow memory\n\n    A-&gt;&gt;A: Combine memories&lt;br/&gt;h_combined\n    A-&gt;&gt;O: Produce logits&lt;br/&gt;next-word prediction</code></pre>"},{"location":"tiny-nested-learning/#example-results-including-real-output","title":"\ud83c\udfc6 Example Results \u2014 Including REAL Output","text":"<pre><code>==================== TRAINING TINY_HOPE ====================\nEpochs per task: 3, Batch size: 64\n\n\ud83d\udcd8 Training tiny_hope\n\u2192 Starting Task_0\n\u2713 Finished Task_0\nEvaluating retention after Task_0...\n\n\ud83d\udcd8 Training tiny_hope\n\u2192 Starting Task_1\n\u2713 Finished Task_1\nEvaluating retention after Task_1...\n\u2713 Completed all tasks for tiny_hope\n</code></pre>"},{"location":"tiny-nested-learning/#transformer-retention-table","title":"\ud83d\udcca Transformer Retention Table","text":"<pre><code>==================== TRANSFORMER RETENTION ====================\nEvaluation Task | After Task_0 | After Task_1 | Forgetting\n---------------------------------------------------------------------------\nTask_0         |      0.975 |      0.800 |   -0.175\nTask_1         |      0.525 |      1.000 |    0.475\n===========================================================================\n</code></pre>"},{"location":"tiny-nested-learning/#hope-retention-table","title":"\ud83d\udcca HOPE Retention Table","text":"<pre><code>==================== HOPE RETENTION ====================\nEvaluation Task | After Task_0 | After Task_1 | Forgetting\n---------------------------------------------------------------------------\nTask_0         |      0.575 |      0.700 |    0.125\nTask_1         |      0.325 |      0.625 |    0.300\n===========================================================================\n</code></pre>"},{"location":"tiny-nested-learning/#continual-learning-summary","title":"\u2b50 Continual Learning Summary","text":"<pre><code>Transformer forget: -0.175\nHOPE forget:        0.125\n\n\ud83d\udc49 HOPE retained more memory (less forgetting).\n</code></pre>"},{"location":"tiny-nested-learning/#educational-explanation-understanding-the-results","title":"\ud83d\udcd8 Educational Explanation: Understanding the Results","text":"<p>Catastrophic forgetting occurs when a model learns Task 1 and overwrites what it learned in Task 0.</p> <p>Final accuracy alone is misleading. The correct metric in continual learning is:</p> <pre><code>FORGETTING = Final Accuracy \u2013 Start Accuracy\n</code></pre> <p>A model with lower forgetting (closer to zero or positive) is the better continual learner.</p>"},{"location":"tiny-nested-learning/#in-this-experiment","title":"In this experiment:","text":"<ul> <li>The Transformer forgot 17.5% of Task_0.</li> <li>HOPE actually improved on Task_0 by 12.5%, meaning zero catastrophic forgetting.</li> </ul> <p>This happens because HOPE uses multi-timescale memory:</p> <ul> <li>Fast memory \u2192 adapts quickly  </li> <li>Medium memory \u2192 blends  </li> <li>Slow memory \u2192 preserves long-term knowledge  </li> </ul> <p>This mirrors Google's Nested Learning idea:</p> <p>Learning at multiple speeds protects older knowledge while adapting to new tasks.</p>"},{"location":"tiny-nested-learning/#key-takeaway","title":"\u2714 Key Takeaway","text":"<pre><code>The better continual learner is the one that FORGETS LESS.\n</code></pre> <p>HOPE wins this experiment.</p>"},{"location":"tiny-nested-learning/#why-hope-works-better-here","title":"\ud83d\udd0d Why HOPE Works Better Here","text":"<ul> <li>Slow memory barely changes \u2192 protects Task 0  </li> <li>Fast memory absorbs Task 1 quickly \u2192 lower interference  </li> <li>Medium memory blends both patterns  </li> <li>Transformer updates one shared weight space, overwriting earlier information  </li> </ul> <p>HOPE demonstrates how multi-timescale memory can significantly reduce catastrophic forgetting.</p>"},{"location":"tiny-nested-learning/#disclaimer-hope-can-also-forget-more","title":"\u26a0\ufe0f Disclaimer \u2014 HOPE Can Also Forget More","text":"<p>To be scientifically honest:</p> <p>HOPE can forget more than a Transformer if:</p> <ul> <li>fast memory rate is too high  </li> <li>slow memory is not slow enough  </li> <li>tasks are extremely different  </li> <li>the model is very tiny  </li> <li>training runs too long  </li> </ul> <p>Example bad setting:</p> <pre><code>fast = 0.95\nmedium = 0.50\nslow = 0.10\n</code></pre> <p>Produces retention like:</p> <pre><code>Transformer retains: 0.82\nHOPE retains:        0.40\n</code></pre> <p>This demonstrates:</p> <p>Multi\u2011timescale memory is powerful only when tuned properly.</p>"},{"location":"tiny-nested-learning/#customize-explore","title":"\ud83d\udd27 Customize &amp; Explore","text":"<p>Try:</p> <ul> <li>Adjusting HOPE update rates  </li> <li>Adding more tasks (Bus \u2192 Flight \u2192 Metro \u2192 Boat)  </li> <li>Increasing vocabulary size  </li> <li>Changing Transformer depth  </li> <li>Lowering Task 1 epochs to reduce destructive updates  </li> </ul>"},{"location":"tiny-nested-learning/#rnn-vs-transformer-vs-hope-quick-comparison","title":"\ud83d\udcd8 RNN vs Transformer vs HOPE (Quick Comparison)","text":"Model Memory Type Strengths Weaknesses RNN One hidden state Simple, sequential Severe forgetting Transformer Shared parameter memory Strong modeling power High forgetting when fine-tuned HOPE Fast + Medium + Slow Protects old tasks via slow memory Needs tuning"},{"location":"tiny-nested-learning/#why-nested-learning","title":"\ud83e\udde0 Why \u201cNested Learning\u201d?","text":"<p>Traditional models update one memory system.</p> <p>Nested Learning updates multiple memory systems simultaneously, each at a different speed:</p> <ul> <li>Fast \u2192 immediate adaptation  </li> <li>Medium \u2192 short\u2011term consolidation  </li> <li>Slow \u2192 long\u2011term stability  </li> </ul> <p>HOPE is a small but functional example of this idea.</p>"},{"location":"tiny-nested-learning/#final-notes","title":"\ud83c\udf89 Final Notes","text":"<p>This project is deliberately tiny \u2014 small enough to understand deeply, but powerful enough to illustrate the most important concepts in continual learning:</p> <ul> <li>Catastrophic forgetting  </li> <li>Multi-timescale memory  </li> <li>Nested Learning  </li> <li>Transformer vs HOPE behavior  </li> </ul> <p>Use it as a learning tool, demo, or foundation for larger experiments.</p>"}]}